# Masked Language Modeling with Attention

## Description
This project implements an **AI that predicts masked words** in English text using a transformer-based language model.  
It uses **BERT** (Bidirectional Encoder Representations from Transformers) to predict masked tokens based on context and generate visualizations of attention patterns.

## Objective
- Predict a `[MASK]` token in a sentence using BERT.  
- Visualize attention scores for all **attention heads and layers** in BERT.  
- Analyze what each attention head focuses on when interpreting language.
